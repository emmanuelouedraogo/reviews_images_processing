{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import cv2\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "# --- Constantes de configuration ---\n",
    "EFFICIENTNET_INPUT_SIZE = (224, 224)\n",
    "PHOTOS_DIR_PATH = 'data/photos/'\n",
    "# --- Chargement des modèles et des données ---\n",
    "@st.cache_resource\n",
    "def charger_modeles():\n",
    "    \"\"\"Charge les modèles et les objets nécessaires.\"\"\"\n",
    "    try:\n",
    "        feature_extractor = EfficientNetB0(include_top=False, weights='imagenet', pooling='avg')\n",
    "        scaler = joblib.load('scaler.joblib')\n",
    "        pca = joblib.load('pca.joblib')\n",
    "        model = joblib.load('random_forest_model.joblib')\n",
    "        label_encoder = joblib.load('label_encoder.joblib')\n",
    "        \n",
    "        # Charger les données d'entraînement pour l'analyse des mots-clés\n",
    "        entrainement_data = joblib.load(\"entrainement.joblib\")\n",
    "        X_train_text = entrainement_data['X_train']\n",
    "        y_train = entrainement_data['y_train']\n",
    "        \n",
    "        return feature_extractor, scaler, pca, model, label_encoder, X_train_text, y_train\n",
    "    except Exception as e:\n",
    "        st.error(f\"Erreur lors du chargement des modèles/données : {e}\")\n",
    "        return None, None, None, None, None, None, None\n",
    "\n",
    "feature_extractor, scaler, pca, model, label_encoder, X_train_text, y_train = charger_modeles()\n",
    "\n",
    "def preprocess_image(image):\n",
    "    \"\"\"Prétraite une image pour EfficientNetB0.\"\"\"\n",
    "    img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    img_resized = cv2.resize(img_rgb, EFFICIENTNET_INPUT_SIZE)\n",
    "    img_expanded = np.expand_dims(img_resized, axis=0)\n",
    "    img_preprocessed = preprocess_input(img_expanded)\n",
    "    return img_preprocessed\n",
    "\n",
    "def extract_image_features(image, model):\n",
    "    \"\"\"Extrait les caractéristiques d'une image.\"\"\"\n",
    "    img_preprocessed = preprocess_image(image)\n",
    "    return model.predict(img_preprocessed, verbose=0)[0]\n",
    "\n",
    "def predict_topic(image, model_obj, scaler_obj, pca_obj):\n",
    "    \"\"\"Prédit le topic d'une image ou d'un texte.\"\"\"\n",
    "    if isinstance(image, np.ndarray):  # Si l'entrée est une image\n",
    "        img_features = extract_image_features(image, feature_extractor)\n",
    "        img_features_scaled = scaler_obj.transform(img_features.reshape(1, -1))\n",
    "        if pca_obj is not None:\n",
    "            img_features_pca = pca_obj.transform(img_features_scaled)\n",
    "        else:\n",
    "            img_features_pca = img_features_scaled\n",
    "        prediction = model_obj.predict(img_features_pca)[0]\n",
    "    elif isinstance(image, str):  # Si l'entrée est du texte\n",
    "        text_features = scaler_obj.transform(image) # scaler est déjà fit sur les features efficientnet\n",
    "        if pca_obj is not None:\n",
    "            text_features_pca = pca_obj.transform(text_features)\n",
    "        else:\n",
    "            text_features_pca = text_features\n",
    "        prediction = model_obj.predict(text_features_pca)[0]\n",
    "    else:\n",
    "        raise ValueError(\"Entrée non valide. Doit être une image (np.ndarray) ou du texte (str).\")\n",
    "    return label_encoder.inverse_transform([prediction])[0]  # Décode la prédiction\n",
    "\n",
    "def get_tsne_visualization(features, labels, new_text_feature=None, new_text_label=None):\n",
    "    \"\"\"\n",
    "    Calcule et affiche la visualisation t-SNE des caractéristiques,\n",
    "    en incluant éventuellement un nouveau point de texte.\n",
    "    \"\"\"\n",
    "    all_features = np.copy(features)  # Copie pour ne pas modifier l'original\n",
    "    all_labels = list(labels)\n",
    "\n",
    "    if new_text_feature is not None and new_text_label is not None:\n",
    "        all_features = np.vstack([all_features, new_text_feature])\n",
    "        all_labels.append(new_text_label)\n",
    "\n",
    "    # Vérification que all_features a au moins 2 éléments\n",
    "    if len(all_features) < 2:\n",
    "        st.warning(\"Pas assez de données pour appliquer t-SNE.\")\n",
    "        return None\n",
    "        \n",
    "    perplexity_value = min(30, len(all_features) - 1)\n",
    "    tsne = manifold.TSNE(n_components=2, init='pca', random_state=42, perplexity=perplexity_value)\n",
    "    X_tsne = tsne.fit_transform(all_features)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Créer un mappage de couleurs basé sur les étiquettes\n",
    "    unique_labels = list(set(all_labels))\n",
    "    palette = sns.color_palette(\"viridis\", n_colors=len(unique_labels))\n",
    "    label_color_map = dict(zip(unique_labels, palette))\n",
    "\n",
    "    # Assigner des couleurs aux points\n",
    "    colors = [label_color_map[label] for label in all_labels]\n",
    "    \n",
    "    sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=all_labels, palette=label_color_map) # Utilisez le mappage ici\n",
    "    \n",
    "    plt.title('Visualisation t-SNE des topics, incluant le nouveau texte')\n",
    "    plt.xlabel('t-SNE Composante 1')\n",
    "    plt.ylabel('t-SNE Composante 2')\n",
    "    plt.legend(title=\"Topic\")\n",
    "\n",
    "    # Convertir le graphique en image pour Streamlit\n",
    "    buf = BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    image_png = buf.getvalue()\n",
    "    buf.close()\n",
    "    plt.close()\n",
    "    \n",
    "    return image_png\n",
    "    \n",
    "def get_top_keywords(corpus, labels, n_keywords=10):\n",
    "    \"\"\"\n",
    "    Extrait les mots-clés les plus importants pour chaque topic à partir du corpus d'entraînement.\n",
    "\n",
    "    Args:\n",
    "        corpus (pd.Series): Le corpus de texte d'entraînement.\n",
    "        labels (pd.Series): Les étiquettes (topics) correspondant au corpus.\n",
    "        n_keywords (int): Le nombre de mots-clés à extraire par topic.\n",
    "\n",
    "    Returns:\n",
    "        dict: Un dictionnaire où les clés sont les topics et les valeurs sont des listes de mots-clés.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    keywords_by_topic = {}\n",
    "    for topic in labels.unique():\n",
    "        # Sélectionne les documents appartenant à ce topic\n",
    "        topic_docs_indices = np.where(labels == topic)[0]\n",
    "        topic_tfidf_matrix = tfidf_matrix[topic_docs_indices]\n",
    "        \n",
    "        # Calcule la somme des scores TF-IDF pour chaque mot dans le topic\n",
    "        sum_tfidf_scores = topic_tfidf_matrix.sum(axis=0).A1  # Convertit en array 1D\n",
    "        \n",
    "        # Obtient les indices des mots avec les scores TF-IDF les plus élevés\n",
    "        top_keyword_indices = sum_tfidf_scores.argsort()[-n_keywords:][::-1]\n",
    "        \n",
    "        # Obtient les mots-clés correspondants\n",
    "        top_keywords = [feature_names[index] for index in top_keyword_indices]\n",
    "        keywords_by_topic[topic] = top_keywords\n",
    "    return keywords_by_topic\n",
    "\n",
    "def summarize_keywords(keywords_dict, n_top=3):\n",
    "    \"\"\"\n",
    "    Génère une description concise des mots-clés pour chaque topic.\n",
    "\n",
    "    Args:\n",
    "        keywords_dict (dict): Dictionnaire de mots-clés par topic.\n",
    "        n_top (int): Nombre de mots-clés principaux à utiliser dans le résumé.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionnaire avec les résumés pour chaque topic.\n",
    "    \"\"\"\n",
    "    summaries = {}\n",
    "    for topic, keywords in keywords_dict.items():\n",
    "        if len(keywords) > n_top:\n",
    "            top_keywords = keywords[:n_top]\n",
    "            summaries[topic] = f\"Principaux thèmes : {', '.join(top_keywords)}\"\n",
    "        elif len(keywords) > 0:\n",
    "            summaries[topic] = f\"Principaux thèmes : {', '.join(keywords)}\"\n",
    "        else:\n",
    "            summaries[topic] = \"Aucun mot-clé pertinent trouvé\"\n",
    "    return summaries\n",
    "\n",
    "def main():\n",
    "    \"\"\"Fonction principale de l'application Streamlit.\"\"\"\n",
    "    st.title(\"Classification de Topics d'Images et de Textes\")\n",
    "    st.write(\"Ce modèle classe des images et des textes dans différents topics.\")\n",
    "\n",
    "    # Sélecteur pour choisir le type d'entrée\n",
    "    input_type = st.radio(\"Type d'entrée :\", [\"Image\", \"Texte\"])\n",
    "\n",
    "    if input_type == \"Image\":\n",
    "        uploaded_file = st.file_uploader(\"Télécharger une image\", type=[\"jpg\", \"png\", \"jpeg\"])\n",
    "        if uploaded_file is not None:\n",
    "            # Lire l'image téléchargée\n",
    "            image = cv2.imdecode(np.frombuffer(uploaded_file.read(), np.uint8), cv2.IMREAD_COLOR)\n",
    "            st.image(image, caption=\"Image téléchargée\", use_column_width=True)\n",
    "            \n",
    "            if st.button(\"Prédire le topic de l'image\"):\n",
    "                try:\n",
    "                    predicted_topic = predict_topic(image, model, scaler, pca)\n",
    "                    st.success(f\"Topic prédit : {predicted_topic}\")\n",
    "                except Exception as e:\n",
    "                    st.error(f\"Erreur lors de la prédiction : {e}\")\n",
    "                    \n",
    "    elif input_type == \"Texte\":\n",
    "        new_text = st.text_area(\"Entrer un texte pour la classification :\", \"C'est un document intéressant.\")\n",
    "        if st.button(\"Prédire le topic du texte\"):\n",
    "            try:\n",
    "                # Prédiction du topic du nouveau texte\n",
    "                new_text_features = scaler.transform(feature_extractor.predict(preprocess_input(np.expand_dims(np.zeros(EFFICIENTNET_INPUT_SIZE, dtype=np.uint8), axis=0)), verbose=0)[0].reshape(1,-1)) # vecteur nul\n",
    "                predicted_topic_text = predict_topic(new_text_features, model, scaler, pca)\n",
    "                st.success(f\"Topic prédit pour le texte : {predicted_topic_text}\")\n",
    "                \n",
    "                # Visualisation t-SNE avec le nouveau point de texte\n",
    "                tsne_image = get_tsne_visualization(np.stack(processed_features_X), processed_labels_y, new_text_features, predicted_topic_text)\n",
    "                if tsne_image:\n",
    "                    st.subheader(\"Visualisation t-SNE avec le texte ajouté\")\n",
    "                    st.image(tsne_image, caption=\"Visualisation t-SNE\", use_column_width=True)\n",
    "                else:\n",
    "                    st.warning(\"La visualisation t-SNE n'a pas pu être générée.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                st.error(f\"Erreur lors de la prédiction : {e}\")\n",
    "\n",
    "    # Analyse des mots-clés par topic\n",
    "    st.header(\"Analyse des mots-clés par topic\")\n",
    "    try:\n",
    "        keywords_by_topic = get_top_keywords(X_train_text, y_train)\n",
    "        summaries = summarize_keywords(keywords_by_topic)\n",
    "        for topic, summary in summaries.items():\n",
    "            st.subheader(f\"Topic : {topic}\")\n",
    "            st.write(summary)\n",
    "    except Exception as e:\n",
    "        st.error(f\"Erreur lors de l'analyse des mots-clés : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
