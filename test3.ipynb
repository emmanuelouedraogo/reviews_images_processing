{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e366fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraînement du pipeline complet...\n",
      "--- Attention : Utilisation de TfidfVectorizer avec LDA ---\n",
      "Entraînement terminé.\n",
      "\n",
      "Prédiction du topic pour le texte : 'The check-in staff took forever and seemed unfriendly.'\n",
      "Distribution de probabilité : [0.10490197 0.11182256 0.10501788 0.6782576 ]\n",
      "Le numéro du topic prédit (0 à 3) est : 3\n",
      "\n",
      "--- Mots clés par topic (appris lors du fit) ---\n",
      "Topic 0: bed, uncomfortable, coffee, variety, cold, breakfast, room\n",
      "Topic 1: restaurant, quick, food, efficient, process, especially, excellent\n",
      "Topic 2: bathroom, hard, poor, shower, clean, hair, face\n",
      "Topic 3: friendly, helpful, staff, great, near, center, city\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emman\\Desktop\\open\\openpy3.12.9\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "# --- Import TfidfVectorizer instead of CountVectorizer ---\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# --- Téléchargement des ressources NLTK nécessaires (si pas déjà fait) ---\n",
    "# Vous pouvez exécuter ces lignes une fois dans votre environnement\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4') # Open Multilingual Wordnet\n",
    "# nltk.download('words') # Pour la liste de mots anglais\n",
    "\n",
    "# --- Définition des Fonctions Auxiliaires ---\n",
    "\n",
    "# Ensemble de mots anglais valides (peut être chargé depuis une source externe si nécessaire)\n",
    "try:\n",
    "    english_words = set(nltk.corpus.words.words())\n",
    "except LookupError:\n",
    "    print(\"NLTK 'words' corpus non trouvé. Téléchargez-le avec nltk.download('words')\")\n",
    "    print(\"Utilisation d'un ensemble vide pour english_words pour continuer l'exemple.\")\n",
    "    english_words = set() # Placeholder si non téléchargé\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "    Convertit les étiquettes de parties du discours (Part of Speech, POS) de Penn Treebank en format WordNet.\n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Par défaut, c'est un nom\n",
    "\n",
    "def preprocess_text(text, english_words_set=english_words, min_len_word=3, rejoin=False):\n",
    "    \"\"\"\n",
    "    Pré-traite un texte : minuscules, suppression non-alpha, stop words, lemmatisation, mots courts.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str): # Vérification de type simple\n",
    "        return \"\" if rejoin else []\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"[^a-z]\", \" \", text)\n",
    "    try:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "    except LookupError:\n",
    "        print(\"NLTK 'stopwords' non trouvé. Téléchargez-le avec nltk.download('stopwords')\")\n",
    "        stop_words = set() # Placeholder\n",
    "\n",
    "    words = [word for word in text.split() if word not in stop_words]\n",
    "    # Utilise l'ensemble de mots passé en argument\n",
    "    filtered_corpus = [word for word in words if word in english_words_set]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    try:\n",
    "        tagged_words = pos_tag(filtered_corpus)\n",
    "    except LookupError:\n",
    "         print(\"NLTK 'averaged_perceptron_tagger' non trouvé. Téléchargez-le.\")\n",
    "         # Sans POS tagging, on lemmatise comme des noms par défaut\n",
    "         tagged_words = [(word, 'N') for word in filtered_corpus]\n",
    "\n",
    "    words = [\n",
    "        lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged_words\n",
    "    ]\n",
    "\n",
    "    final_tokens = [w for w in words if len(w) >= min_len_word] # Corrected: >= instead of &gt;=\n",
    "\n",
    "    if rejoin:\n",
    "        return \" \".join(final_tokens)\n",
    "\n",
    "    return final_tokens\n",
    "\n",
    "# Note: clean_dataset est redondant si on utilise TextPreprocessor avec rejoin=True\n",
    "\n",
    "def topic_modeling_lda(reviews, num_topics):\n",
    "    \"\"\"\n",
    "    Vectorise (TfidfVectorizer) et crée le modèle LDA. Retourne modèle, vectorizer, vecteurs de sujet, matrice TF-IDF.\n",
    "    *** ATTENTION : LDA est généralement conçu pour fonctionner avec des comptes de mots (CountVectorizer).\n",
    "    *** L'utilisation de TF-IDF comme entrée pour LDA est moins courante et peut donner des résultats\n",
    "    *** moins interprétables car le modèle probabiliste sous-jacent de LDA est basé sur des comptes.\n",
    "    \"\"\"\n",
    "    print(\"--- Attention : Utilisation de TfidfVectorizer avec LDA ---\")\n",
    "    # --- Utilisation de TfidfVectorizer ---\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_df=0.95, min_df=1, max_features=1000, stop_words=\"english\"\n",
    "        # Attention: stop_words='english' ici peut être redondant si preprocess_text les a déjà enlevés.\n",
    "        # Si preprocess_text les enlève, mettez stop_words=None ici.\n",
    "    )\n",
    "    # --- La matrice résultante est une matrice TF-IDF ---\n",
    "    tfidf_matrix = vectorizer.fit_transform(reviews)\n",
    "\n",
    "    lda_model = LatentDirichletAllocation(\n",
    "        n_components=num_topics, learning_method=\"online\", random_state=0, max_iter=10 # max_iter peut nécessiter ajustement\n",
    "    )\n",
    "\n",
    "    # Gérer le cas où la matrice tf-idf est vide après vectorisation\n",
    "    if tfidf_matrix.shape[0] == 0 or tfidf_matrix.shape[1] == 0:\n",
    "         print(\"Attention : La matrice TF-IDF est vide après vectorisation. LDA ne peut pas être entraîné.\")\n",
    "         # Retourner des valeurs par défaut ou lever une exception\n",
    "         return None, vectorizer, np.array([]), tfidf_matrix\n",
    "\n",
    "    # --- Entraînement de LDA sur la matrice TF-IDF (non standard) ---\n",
    "    lda_model.fit(tfidf_matrix)\n",
    "    topic_vectors = lda_model.transform(tfidf_matrix)\n",
    "\n",
    "    # --- Retourne la matrice TF-IDF au lieu de la matrice de comptage ---\n",
    "    return lda_model, vectorizer, topic_vectors, tfidf_matrix\n",
    "\n",
    "# --- Définition des Classes Transformer pour le Pipeline ---\n",
    "\n",
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformateur personnalisé pour le prétraitement de texte dans un pipeline sklearn.\n",
    "    \"\"\"\n",
    "    def __init__(self, english_words_set, min_len_word=3, rejoin=True):\n",
    "        self.english_words_set = english_words_set\n",
    "        self.min_len_word = min_len_word\n",
    "        self.rejoin = rejoin # Important: doit être True pour que la sortie soit une chaîne pour TfidfVectorizer\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Ce préprocesseur n'a pas besoin d'apprendre quoi que ce soit du set d'entraînement\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # X est attendu comme un itérable de documents (ex: liste de strings, pd.Series)\n",
    "        return [\n",
    "            preprocess_text(doc, self.english_words_set, self.min_len_word, self.rejoin)\n",
    "            for doc in X\n",
    "        ]\n",
    "\n",
    "class TopicModeler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformateur personnalisé pour la modélisation de sujet LDA dans un pipeline sklearn.\n",
    "    Utilise maintenant TfidfVectorizer en interne.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_topics=4):\n",
    "        self.num_topics = num_topics\n",
    "        self.lda_model = None\n",
    "        # --- Renommer pour refléter l'utilisation de TF-IDF ---\n",
    "        self.tfidf_vectorizer = None\n",
    "        self.topic_vectors = None\n",
    "        # --- Renommer pour refléter la matrice TF-IDF ---\n",
    "        self.dtm = None # Document Term Matrix (même si c'est TF-IDF ici)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # X est attendu comme une liste de textes *prétraités* (sortie de TextPreprocessor)\n",
    "        result = topic_modeling_lda(X, self.num_topics)\n",
    "        if result[0] is not None: # Vérifier si LDA a pu être entraîné\n",
    "            # --- Stocker le TfidfVectorizer et la matrice TF-IDF ---\n",
    "            self.lda_model, self.tfidf_vectorizer, self.topic_vectors, self.dtm = result\n",
    "        else:\n",
    "            # Gérer le cas où l'entraînement a échoué (matrice vide)\n",
    "            print(\"Échec de l'entraînement du TopicModeler.\")\n",
    "            self.lda_model = None\n",
    "            self.tfidf_vectorizer = result[1] # Le vectorizer est quand même créé\n",
    "            self.topic_vectors = np.array([])\n",
    "            self.dtm = result[3]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # X est attendu comme une liste de textes *prétraités* (sortie de TextPreprocessor)\n",
    "        # Important: Utilise le vectorizer et le modèle LDA *déjà entraînés* (fittés)\n",
    "        # --- Vérifier si le TfidfVectorizer est entraîné ---\n",
    "        if self.lda_model is None or self.tfidf_vectorizer is None:\n",
    "             print(\"Erreur: Le TopicModeler n'a pas été entraîné correctement.\")\n",
    "             num_samples = len(X) if isinstance(X, list) else X.shape[0]\n",
    "             return np.zeros((num_samples, self.num_topics))\n",
    "\n",
    "        # 1. Vectoriser les nouvelles données avec le TfidfVectorizer entraîné\n",
    "        tfidf_matrix_new = self.tfidf_vectorizer.transform(X)\n",
    "        # 2. Obtenir la distribution de topics avec le modèle LDA entraîné\n",
    "        #    (Note: transforme basé sur la matrice TF-IDF, ce qui est non standard pour LDA)\n",
    "        topic_distribution = self.lda_model.transform(tfidf_matrix_new)\n",
    "        return topic_distribution\n",
    "\n",
    "# --- Création et Utilisation du Pipeline ---\n",
    "\n",
    "# Définir le nombre de topics\n",
    "num_topics = 4\n",
    "\n",
    "# Créer l'instance du pipeline\n",
    "text_pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"preprocessing\",\n",
    "            TextPreprocessor(english_words_set=english_words, min_len_word=3, rejoin=True),\n",
    "        ),\n",
    "        # La sortie de 'preprocessing' est une liste de strings nettoyées\n",
    "        # Cette sortie est l'entrée pour 'topic_modeling'\n",
    "        # 'topic_modeling' utilise maintenant TfidfVectorizer en interne\n",
    "        (\"topic_modeling\", TopicModeler(num_topics=num_topics))\n",
    "        # La sortie de 'topic_modeling' est la distribution de probabilité sur les topics\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- Exemple d'utilisation ---\n",
    "\n",
    "# 1. Entraîner le pipeline sur un corpus complet\n",
    "corpus_complet = [\n",
    "    \"The customer service was terrible and slow, really bad experience.\",\n",
    "    \"Room was clean but the bed was uncomfortable.\",\n",
    "    \"Breakfast lacked variety and coffee was cold.\",\n",
    "    \"Staff were very friendly and helpful during check-in.\",\n",
    "    \"Great location near the city center and attractions.\",\n",
    "    \"Food at the hotel restaurant was excellent, especially the pasta.\",\n",
    "    \"Bathroom wasn't very clean, found hair in the shower.\",\n",
    "    \"Hotel was hard to find, poor signage and location.\",\n",
    "    \"Very noisy room facing the street, couldn't sleep well.\",\n",
    "    \"Quick check out process, efficient staff.\"\n",
    "]\n",
    "\n",
    "print(\"Entraînement du pipeline complet...\")\n",
    "try:\n",
    "    text_pipeline.fit(corpus_complet)\n",
    "    print(\"Entraînement terminé.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur pendant l'entraînement du pipeline : {e}\")\n",
    "    # Arrêter si l'entraînement échoue - Peut-être ajouter un sys.exit(1) ou autre gestion d'erreur\n",
    "\n",
    "# 2. Utiliser le pipeline entraîné pour prédire le topic d'un nouveau texte\n",
    "texte_unique_raw = \"The check-in staff took forever and seemed unfriendly.\"\n",
    "review0_iterable = [texte_unique_raw] # Doit être un itérable (liste ou Series)\n",
    "\n",
    "print(f\"\\nPrédiction du topic pour le texte : '{texte_unique_raw}'\")\n",
    "\n",
    "try:\n",
    "    # Obtenir la distribution de probabilité des topics pour le texte unique\n",
    "    topic_distribution = text_pipeline.transform(review0_iterable)\n",
    "\n",
    "    # Trouver le numéro du topic le plus probable (index du max)\n",
    "    numero_topic_predit = np.argmax(topic_distribution, axis=1)[0]\n",
    "\n",
    "    print(f\"Distribution de probabilité : {topic_distribution[0]}\")\n",
    "    print(f\"Le numéro du topic prédit (0 à {num_topics-1}) est : {numero_topic_predit}\")\n",
    "\n",
    "    # Optionnel : Afficher les mots clés pour interpréter les topics (si l'entraînement a réussi)\n",
    "    if hasattr(text_pipeline.named_steps['topic_modeling'], 'lda_model') and \\\n",
    "       text_pipeline.named_steps['topic_modeling'].lda_model is not None:\n",
    "        print(\"\\n--- Mots clés par topic (appris lors du fit) ---\")\n",
    "        # --- Accéder au TfidfVectorizer ---\n",
    "        vectorizer = text_pipeline.named_steps['topic_modeling'].tfidf_vectorizer\n",
    "        lda_model = text_pipeline.named_steps['topic_modeling'].lda_model\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        n_top_words = 7\n",
    "        for topic_idx, topic_loadings in enumerate(lda_model.components_):\n",
    "            top_words_indices = topic_loadings.argsort()[:-n_top_words - 1:-1]\n",
    "            top_words = [feature_names[i] for i in top_words_indices]\n",
    "            print(f\"Topic {topic_idx}: {', '.join(top_words)}\")\n",
    "    else:\n",
    "        print(\"\\nImpossible d'afficher les mots clés (modèle non entraîné correctement).\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erreur pendant la transformation/prédiction : {e}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openpy3.12.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
