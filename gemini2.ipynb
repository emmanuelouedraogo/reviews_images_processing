{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.6' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import streamlit as st  # Pour l'application Streamlit\n",
    "\n",
    "# Imports TensorFlow/Keras\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "import tensorflow as tf\n",
    "\n",
    "# Imports Scikit-learn\n",
    "from sklearn import manifold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Constantes de configuration ---\n",
    "PHOTOS_JSON_PATH = 'data/photos.json'\n",
    "PHOTOS_DIR_PATH = 'data/photos/'\n",
    "N_SAMPLES_PER_CLASS = 500\n",
    "TARGET_COLUMN = 'label'\n",
    "EFFICIENTNET_INPUT_SIZE = (224, 224)\n",
    "PCA_N_COMPONENTS = 10\n",
    "TEST_SIZE_SPLIT = 0.33\n",
    "RANDOM_STATE_GLOBAL = 42\n",
    "\n",
    "# Paths for saving/loading models\n",
    "SCALER_PATH = 'scaler.joblib'\n",
    "PCA_PATH = 'pca.joblib'\n",
    "MODEL_PATH = 'random_forest_model.joblib'\n",
    "LABEL_ENCODER_PATH = 'label_encoder.joblib'\n",
    "TSNE_PLOT_IMAGE_PATH = 'tsne_visualization.png'\n",
    "FEATURE_EXTRACTOR_PATH = 'feature_extractor.joblib' # Pour sauvegarder EfficientNetB0\n",
    "\n",
    "\n",
    "# --- 1. Chargement et préparation des données ---\n",
    "print(\"1. Chargement et préparation des données...\")\n",
    "\n",
    "def load_data_from_jsonl(file_path):\n",
    "    \"\"\"Charge les données depuis un fichier JSON Lines.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Le chemin du fichier JSON Lines.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Les données chargées, ou un DataFrame vide en cas d'erreur.\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                data_list.append(json.loads(line))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erreur : Le fichier {file_path} n'a pas été trouvé.\")\n",
    "        return pd.DataFrame()\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Erreur de décodage JSON dans {file_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur inattendue s'est produite lors du chargement de {file_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return pd.DataFrame(data_list)\n",
    "\n",
    "\n",
    "df_full = load_data_from_jsonl(PHOTOS_JSON_PATH)\n",
    "\n",
    "if df_full.empty:\n",
    "    print(\"Aucune donnée chargée. Arrêt du script.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "def stratified_sample_df(df: pd.DataFrame, col: str, n_samples: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Effectue un échantillonnage stratifié sur un DataFrame.\n",
    "    S'assure que chaque classe a au moins n_samples, sinon prend le minimum disponible.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Le DataFrame à échantillonner.\n",
    "        col (str): La colonne à utiliser pour la stratification.\n",
    "        n_samples (int): Le nombre d'échantillons souhaité par classe.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Un DataFrame échantillonné.\n",
    "    \"\"\"\n",
    "    min_samples_in_any_class = df[col].value_counts().min()\n",
    "    actual_n_samples = min(n_samples, min_samples_in_any_class)\n",
    "    if actual_n_samples < n_samples:\n",
    "        print(f\"Avertissement : Le nombre d'échantillons demandé ({n_samples}) est supérieur au nombre \"\n",
    "              f\"d'échantillons disponibles dans la plus petite classe ({min_samples_in_any_class}). \"\n",
    "              f\"Utilisation de {actual_n_samples} échantillons par classe.\")\n",
    "\n",
    "    try:\n",
    "        df_sampled = df.groupby(col, group_keys=False).apply(\n",
    "            lambda x: x.sample(n=actual_n_samples, random_state=RANDOM_STATE_GLOBAL))\n",
    "        return df_sampled.reset_index(drop=True)\n",
    "    except KeyError:\n",
    "        print(f\"Erreur : La colonne '{col}' n'existe pas dans le DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur s'est produite lors de l'échantillonnage stratifié : {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "subsampled_df = stratified_sample_df(df_full, TARGET_COLUMN, N_SAMPLES_PER_CLASS)\n",
    "if subsampled_df.empty:\n",
    "    print(\"Aucun échantillon stratifié n'a pu être créé. Arrêt du script.\")\n",
    "    exit()\n",
    "print(f\"Nombre d'échantillons après sous-échantillonnage stratifié : {len(subsampled_df)}\")\n",
    "print(f\"Distribution des classes après échantillonnage :\\n{subsampled_df[TARGET_COLUMN].value_counts()}\")\n",
    "\n",
    "# --- 2. Extraction des caractéristiques (Features) ---\n",
    "print(\"\\n2. Extraction des caractéristiques avec EfficientNetB0...\")\n",
    "\n",
    "# Chargement du modèle pré-entraîné EfficientNetB0 pour l'extraction de caractéristiques\n",
    "try:\n",
    "    feature_extractor = EfficientNetB0(include_top=False, weights='imagenet', pooling='avg')\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors du chargement du modèle EfficientNetB0 : {e}\")\n",
    "    exit()\n",
    "\n",
    "def extract_features(image_path, model, target_size):\n",
    "    \"\"\"\n",
    "    Extrait les caractéristiques d'une image avec un modèle donné.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Chemin vers l'image.\n",
    "        model: Modèle Keras pour l'extraction des caractéristiques.\n",
    "        target_size (tuple): Taille de l'image cible.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Vecteur de caractéristiques extrait, ou None en cas d'erreur.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        image_array = cv2.imread(image_path)\n",
    "        if image_array is None:\n",
    "            raise ValueError(f\"Impossible de charger l'image depuis {image_path}\")\n",
    "\n",
    "        img_rgb = cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB)\n",
    "        img_resized = cv2.resize(img_rgb, target_size)\n",
    "        img_expanded = np.expand_dims(img_resized, axis=0)\n",
    "        img_preprocessed = preprocess_input(img_expanded)\n",
    "\n",
    "        return model.predict(img_preprocessed, verbose=0)[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'extraction des caractéristiques de l'image {image_path}: {e}\")\n",
    "        return None\n",
    "    \n",
    "processed_photo_ids = []\n",
    "processed_features_X = []\n",
    "processed_labels_y = []\n",
    "\n",
    "for index, row in tqdm(subsampled_df.iterrows(), total=subsampled_df.shape[0], desc=\"Extraction des Features\"):\n",
    "    photo_id = row['photo_id']\n",
    "    label = row[TARGET_COLUMN]\n",
    "    image_path = os.path.join(PHOTOS_DIR_PATH, photo_id + '.jpg')\n",
    "\n",
    "    feature_vector = extract_features(image_path, feature_extractor, EFFICIENTNET_INPUT_SIZE)\n",
    "    if feature_vector is not None:\n",
    "        processed_photo_ids.append(photo_id)\n",
    "        processed_features_X.append(feature_vector)\n",
    "        processed_labels_y.append(label)\n",
    "\n",
    "if not processed_features_X:\n",
    "    print(\"Erreur critique : Aucune caractéristique n'a été extraite avec succès. Le script va s'arrêter.\")\n",
    "    exit(1)\n",
    "\n",
    "# Conversion en tableaux NumPy pour scikit-learn\n",
    "features_X_np = np.stack(processed_features_X)\n",
    "labels_y_np = np.array(processed_labels_y)\n",
    "print(f\"\\n{len(features_X_np)} caractéristiques extraites avec succès.\")\n",
    "\n",
    "# Initialisation du LabelEncoder.\n",
    "label_encoder = None\n",
    "# Encodage des étiquettes si elles sont de type chaîne de caractères\n",
    "if labels_y_np.ndim > 0 and labels_y_np.dtype.kind in ['O', 'S', 'U']:\n",
    "    print(\"Encodage des étiquettes (labels) car elles sont de type chaîne de caractères.\")\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels_y_for_model = label_encoder.fit_transform(labels_y_np)\n",
    "    label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "    print(f\"Mappage des étiquettes : {label_mapping}\")\n",
    "else:\n",
    "    labels_y_for_model = labels_y_np\n",
    "\n",
    "# --- 3. Réduction de dimensionnalité avec t-SNE et Visualisation ---\n",
    "print(\"\\n3. Réduction de dimensionnalité avec t-SNE et Visualisation...\")\n",
    "# Vérification que features_X_np a au moins 2 éléments\n",
    "if len(features_X_np) < 2:\n",
    "    print(\"Pas assez d'échantillons pour appliquer t-SNE. La visualisation sera ignorée.\")\n",
    "else:\n",
    "    # Ajustement de la perplexité pour t-SNE si le nombre d'échantillons est faible\n",
    "    perplexity_value = min(30, len(features_X_np) - 1)\n",
    "    try:\n",
    "        tsne = manifold.TSNE(n_components=2, init='pca', random_state=RANDOM_STATE_GLOBAL,\n",
    "                               perplexity=perplexity_value)\n",
    "        X_tsne = tsne.fit_transform(features_X_np)\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=labels_y_np, palette=\"viridis\")\n",
    "        plt.title('Visualisation t-SNE des caractéristiques des images')\n",
    "        plt.xlabel('t-SNE Composante 1')\n",
    "        plt.ylabel('t-SNE Composante 2')\n",
    "        plt.legend(title=TARGET_COLUMN)\n",
    "        plt.show()\n",
    "\n",
    "        plt.savefig(TSNE_PLOT_IMAGE_PATH)\n",
    "        plt.close()\n",
    "        print(f\"Visualisation t-SNE sauvegardée dans {TSNE_PLOT_IMAGE_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'exécution de t-SNE ou de la visualisation : {e}\")\n",
    "\n",
    "# --- 4. Apprentissage d'un classifieur ---\n",
    "print(\"\\n4. Apprentissage d'un classifieur Random Forest...\")\n",
    "\n",
    "# Division des données (en utilisant les features et labels correctement alignés)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features_X_np, labels_y_for_model,\n",
    "    test_size=TEST_SIZE_SPLIT,\n",
    "    random_state=RANDOM_STATE_GLOBAL,\n",
    "    stratify=labels_y_for_model\n",
    ")\n",
    "\n",
    "print(f\"Taille de l'ensemble d'entraînement : {X_train.shape[0]}, Test : {X_test.shape[0]}\")\n",
    "\n",
    "# Standardisation des données\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Réduction de dimensionnalité avec PCA\n",
    "print(f\"Application de la PCA avec {PCA_N_COMPONENTS} composantes...\")\n",
    "actual_pca_n_components = min(PCA_N_COMPONENTS, X_train_std.shape[1], X_train_std.shape[0])\n",
    "if actual_pca_n_components < 1:\n",
    "    actual_pca_n_components = 1\n",
    "try:\n",
    "    pca = PCA(n_components=actual_pca_n_components)\n",
    "    X_train_pca = pca.fit_transform(X_train_std)\n",
    "    X_test_pca = pca.transform(X_test_std)\n",
    "    print(f\"Variance expliquée par les {pca.n_components_} composantes PCA : {np.sum(pca.explained_variance_ratio_):.2f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors de l'application de PCA : {e}\")\n",
    "    X_train_pca = X_train_std\n",
    "    X_test_pca = X_test_std\n",
    "    pca = None\n",
    "\n",
    "# Recherche des meilleurs hyperparamètres pour RandomForestClassifier avec GridSearchCV\n",
    "print(\"Recherche des meilleurs hyperparamètres pour RandomForestClassifier...\")\n",
    "params_rf = {'max_depth': range(5, 20, 3), 'n_estimators': [100, 200, 300]}\n",
    "rf_classifier_model = RandomForestClassifier(random_state=RANDOM_STATE_GLOBAL)\n",
    "\n",
    "cv_folds = 3\n",
    "if len(np.unique(y_train)) > 1 and len(y_train) > 0:\n",
    "    min_samples_per_class_train = np.min(np.bincount(y_train))\n",
    "    cv_folds = min(cv_folds, min_samples_per_class_train)\n",
    "if cv_folds < 2:\n",
    "    cv_folds = 2\n",
    "if X_train_pca.shape[0] < cv_folds:\n",
    "    cv_folds = X_train_pca.shape[0]\n",
    "\n",
    "try:\n",
    "    grid_search_clf = GridSearchCV(rf_classifier_model, params_rf, cv=cv_folds, scoring='accuracy')\n",
    "    grid_search_clf.fit(X_train_pca, y_train)\n",
    "\n",
    "    print(f\"Meilleurs hyperparamètres trouvés : {grid_search_clf.best_params_}\")\n",
    "\n",
    "    # Évaluation du meilleur modèle\n",
    "    best_rf_model = grid_search_clf.best_estimator_\n",
    "    score = best_rf_model.score(X_test_pca, y_test)\n",
    "    print(f\"Score du classifieur (Random Forest) sur l'ensemble de test : {score:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors de la recherche des hyperparamètres ou de l'évaluation du modèle: {e}\")\n",
    "    best_rf_model = None\n",
    "\n",
    "# --- 5. Sauvegarde des modèles et transformateurs entraînés ---\n",
    "print(\"\\nSauvegarde des modèles et transformateurs entraînés...\")\n",
    "try:\n",
    "    joblib.dump(scaler, SCALER_PATH)\n",
    "    print(f\"Scaler sauvegardé dans {SCALER_PATH}\")\n",
    "    if pca is not None:\n",
    "        joblib.dump(pca, PCA_PATH)\n",
    "        print(f\"PCA sauvegardé dans {PCA_PATH}\")\n",
    "    if best_rf_model is not None:\n",
    "        joblib.dump(best_rf_model, MODEL_PATH)\n",
    "        print(f\"Modèle RandomForest sauvegardé dans {MODEL_PATH}\")\n",
    "    if label_encoder:\n",
    "        joblib.dump(label_encoder, LABEL_ENCODER_PATH)\n",
    "        print(f\"LabelEncoder sauvegardé dans {LABEL_ENCODER_PATH}\")\n",
    "    joblib.dump(feature_extractor, FEATURE_EXTRACTOR_PATH) # Sauvegarde du modèle EfficientNet\n",
    "    print(f\"EfficientNetB0 sauvegardé dans {FEATURE_EXTRACTOR_PATH}\")\n",
    "    print(\"Sauvegarde terminée.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors de la sauvegarde des modèles : {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def creer_image_factice(chemin, taille):\n",
    "    \"\"\"Crée une image factice noire si elle n'existe pas déjà.\"\"\"\n",
    "    if not os.path.exists(chemin):\n",
    "        os.makedirs(os.path.dirname(chemin), exist_ok=True)\n",
    "        cv2.imwrite(chemin, np.zeros((taille[0], taille[1], 3), dtype=np.uint8))\n",
    "        print(f\"Image factice créée : {chemin}\")\n",
    "    return chemin\n",
    "\n",
    "# --- 7. Fonction Principale ---\n",
    "def main():\n",
    "    \"\"\"Fonction principale pour exécuter le pipeline d'extraction de caractéristiques et d'entraînement.\"\"\"\n",
    "    # --- 1. Chargement et préparation des données ---\n",
    "    df_full = load_data_from_jsonl(PHOTOS_JSON_PATH)\n",
    "    if df_full.empty:\n",
    "        return\n",
    "\n",
    "    subsampled_df = stratified_sample_df(df_full, TARGET_COLUMN, N_SAMPLES_PER_CLASS)\n",
    "    if subsampled_df.empty:\n",
    "        return\n",
    "\n",
    "    print(f\"Nombre d'échantillons après sous-échantillonnage stratifié : {len(subsampled_df)}\")\n",
    "    print(f\"Distribution des classes après échantillonnage :\\n{subsampled_df[TARGET_COLUMN].value_counts()}\")\n",
    "\n",
    "    # --- 2. Extraction des caractéristiques ---\n",
    "    print(\"\\n2. Extraction des caractéristiques avec EfficientNetB0...\")\n",
    "    try:\n",
    "        feature_extractor = EfficientNetB0(include_top=False, weights='imagenet', pooling='avg')\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement du modèle EfficientNetB0 : {e}\")\n",
    "        return\n",
    "\n",
    "    processed_photo_ids = []\n",
    "    processed_features_X = []\n",
    "    processed_labels_y = []\n",
    "\n",
    "    for index, row in tqdm(subsampled_df.iterrows(), total=subsampled_df.shape[0],\n",
    "                            desc=\"Extraction des caractéristiques\"):\n",
    "        photo_id = row['photo_id']\n",
    "        label = row[TARGET_COLUMN]\n",
    "        image_path = os.path.join(PHOTOS_DIR_PATH, photo_id + '.jpg')\n",
    "        feature_vector = extract_features(image_path, feature_extractor, EFFICIENTNET_INPUT_SIZE)\n",
    "\n",
    "        if feature_vector is not None:\n",
    "            processed_photo_ids.append(photo_id)\n",
    "            processed_features_X.append(feature_vector)\n",
    "            processed_labels_y.append(label)\n",
    "\n",
    "    if not processed_features_X:\n",
    "        print(\"Erreur critique : Aucune caractéristique n'a été extraite. Arrêt.\")\n",
    "        return\n",
    "\n",
    "    features_X_np = np.stack(processed_features_X)\n",
    "    labels_y_np = np.array(processed_labels_y)\n",
    "    print(f\"\\n{len(features_X_np)} caractéristiques extraites avec succès.\")\n",
    "\n",
    "    # --- 3. Réduction de dimensionnalité et visualisation ---\n",
    "    print(\"\\n3. Réduction de dimensionnalité avec t-SNE et visualisation...\")\n",
    "    if len(features_X_np) >= 2:\n",
    "        perplexity_value = min(30, len(features_X_np) - 1)\n",
    "        try:\n",
    "            tsne = manifold.TSNE(n_components=2, init='pca', random_state=RANDOM_STATE_GLOBAL,\n",
    "                                    perplexity=perplexity_value)\n",
    "            X_tsne = tsne.fit_transform(features_X_np)\n",
    "\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=labels_y_np, palette=\"viridis\")\n",
    "            plt.title('Visualisation t-SNE des caractéristiques des images')\n",
    "            plt.xlabel('t-SNE Composante 1')\n",
    "            plt.ylabel('t-SNE Composante 2')\n",
    "            plt.legend(title=TARGET_COLUMN)\n",
    "            plt.show()\n",
    "            plt.savefig(TSNE_PLOT_IMAGE_PATH)\n",
    "            plt.close()\n",
    "            print(f\"Visualisation t-SNE sauvegardée dans {TSNE_PLOT_IMAGE_PATH}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de la visualisation t-SNE : {e}\")\n",
    "    else:\n",
    "        print(\"Nombre d'échantillons insuffisant pour t-SNE. Visualisation ignorée.\")\n",
    "\n",
    "    # --- 4. Apprentissage du classifieur ---\n",
    "    print(\"\\n4. Apprentissage d'un classifieur Random Forest...\")\n",
    "    label_encoder = None\n",
    "    if labels_y_np.ndim > 0 and labels_y_np.dtype.kind in ['O', 'S', 'U']:\n",
    "        label_encoder = LabelEncoder()\n",
    "        labels_y_for_model = label_encoder.fit_transform(labels_y_np)\n",
    "    else:\n",
    "        labels_y_for_model = labels_y_np\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features_X_np, labels_y_for_model,\n",
    "        test_size=TEST_SIZE_SPLIT,\n",
    "        random_state=RANDOM_STATE_GLOBAL,\n",
    "        stratify=labels_y_for_model\n",
    "    )\n",
    "\n",
    "    print(f\"Taille de l'ensemble d'entraînement : {X_train.shape[0]}, Test : {X_test.shape[0]}\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_std = scaler.fit_transform(X_train)\n",
    "    X_test_std = scaler.transform(X_test)\n",
    "\n",
    "    actual_pca_n_components = min(PCA_N_COMPONENTS, X_train_std.shape[1], X_train_std.shape[0])\n",
    "    if actual_pca_n_components < 1:\n",
    "        actual_pca_n_components = 1\n",
    "    try:\n",
    "        pca = PCA(n_components=actual_pca_n_components)\n",
    "        X_train_pca = pca.fit_transform(X_train_std)\n",
    "        X_test_pca = pca.transform(X_test_std)\n",
    "        print(f\"Variance expliquée : {np.sum(pca.explained_variance_ratio_):.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'application de PCA : {e}\")\n",
    "        X_train_pca = X_train_std\n",
    "        X_test_pca = X_test_std\n",
    "        pca = None\n",
    "\n",
    "    params_rf = {'max_depth': range(5, 20, 3), 'n_estimators': [100, 200, 300]}\n",
    "    rf_classifier_model = RandomForestClassifier(random_state=RANDOM_STATE_GLOBAL)\n",
    "    cv_folds = 3\n",
    "    if len(np.unique(y_train)) > 1 and len(y_train) > 0:\n",
    "        min_samples_per_class_train = np.min(np.bincount(y_train))\n",
    "        cv_folds = min(cv_folds, min_samples_per_class_train)\n",
    "    if cv_folds < 2:\n",
    "        cv_folds = 2\n",
    "    if X_train_pca.shape[0] < cv_folds:\n",
    "        cv_folds = X_train_pca.shape[0]\n",
    "\n",
    "    try:\n",
    "        grid_search_clf = GridSearchCV(rf_classifier_model, params_rf, cv=cv_folds,\n",
    "                                            scoring='accuracy')\n",
    "        grid_search_clf.fit(X_train_pca, y_train)\n",
    "        print(f\"Meilleurs paramètres : {grid_search_clf.best_params_}\")\n",
    "        best_rf_model = grid_search_clf.best_estimator_\n",
    "        score = best_rf_model.score(X_test_pca, y_test)\n",
    "        print(f\"Score sur l'ensemble de test : {score:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'entraînement du modèle : {e}\")\n",
    "        best_rf_model = None\n",
    "\n",
    "    # --- 5. Sauvegarde des modèles ---\n",
    "    print(\"\\n5. Sauvegarde des modèles et des transformateurs...\")\n",
    "    try:\n",
    "        joblib.dump(scaler, SCALER_PATH)\n",
    "        print(f\"Scaler sauvegardé : {SCALER_PATH}\")\n",
    "        if pca is not None:\n",
    "            joblib.dump(pca, PCA_PATH)\n",
    "            print(f\"PCA sauvegardé : {PCA_PATH}\")\n",
    "        if best_rf_model is not None:\n",
    "            joblib.dump(best_rf_model, MODEL_PATH)\n",
    "            print(f\"Modèle sauvegardé : {MODEL_PATH}\")\n",
    "        if label_encoder:\n",
    "            joblib.dump(label_encoder, LABEL_ENCODER_PATH)\n",
    "            print(f\"LabelEncoder sauvegardé : {LABEL_ENCODER_PATH}\")\n",
    "        joblib.dump(feature_extractor, FEATURE_EXTRACTOR_PATH) # Save EfficientNet model\n",
    "        print(f\"EfficientNetB0 sauvegardé : {FEATURE_EXTRACTOR_PATH}\")\n",
    "        print(\"Sauvegarde terminée.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la sauvegarde : {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 6. Exemple de prédiction ---\n",
    "    print(\"\\n6. Exemple de prédiction sur une nouvelle image...\")\n",
    "    dummy_new_image_path = creer_image_factice(os.path.join(PHOTOS_DIR_PATH, \"dummy_new_image.jpg\"),\n",
    "                                            EFFICIENTNET_INPUT_SIZE)\n",
    "    if os.path.exists(dummy_new_image_path):\n",
    "        predicted_topic = predict_image_topic(dummy_new_image_path,\n",
    "                                                feature_extractor,\n",
    "                                                scaler,\n",
    "                                                pca,\n",
    "                                                best_rf_model,\n",
    "                                                label_encoder)\n",
    "        if predicted_topic:\n",
    "            print(f\"Topic prédit pour '{dummy_new_image_path}': {predicted_topic}\")\n",
    "    else:\n",
    "        print(f\"L'image de test {dummy_new_image_path} n'a pas pu être créée. Test de prédiction ignoré.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
