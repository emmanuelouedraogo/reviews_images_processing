# Importation des libraries Streamlit et Core
import nltk.downloader
import streamlit as st
import numpy as np
import pandas as pd
import re
import os
import logging
from collections import Counter

# Importations NLTK (avec gestion des t√©l√©chargements)
import nltk
from nltk.corpus import stopwords, wordnet, words
from nltk import pos_tag
from nltk.stem import WordNetLemmatizer

# Importations Scikit-learn
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.manifold import TSNE

# Importations Visualisation
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go # Pour ajouter des traces sp√©cifiques
from wordcloud import WordCloud

# --- Configuration et T√©l√©chargements NLTK (Mise en cache) ---
# Utiliser cache_resource pour les t√©l√©chargements qui ne changent pas
@st.cache_resource
def download_nltk_data():
    """T√©l√©charge les ressources NLTK n√©cessaires."""
    resources = ["stopwords", "words", "punkt", "wordnet", "averaged_perceptron_tagger"]
    for resource in resources:
        try:
            nltk.data.find(f"tokenizers/{resource}")
        except : # Correction: Utiliser nltk.DownloadError directement
             try: # Essayer de trouver dans les corpora
                 nltk.data.find(f"corpora/{resource}")
             except: # Sinon t√©l√©charger
                 st.info(f"T√©l√©chargement de la ressource NLTK : {resource}")
                 nltk.download(resource, quiet=True)
    return True

# Assurer le t√©l√©chargement au d√©marrage
NLTK_DATA_LOADED = download_nltk_data()

# --- Initialisation des variables globales (apr√®s t√©l√©chargement) ---
if NLTK_DATA_LOADED:
    stop_words_set = set(stopwords.words("english"))
    english_words_set = set(words.words())
else:
    st.error("Impossible de charger les ressources NLTK. L'application risque de ne pas fonctionner.")
    st.stop() # Arr√™ter l'ex√©cution si NLTK √©choue

# --- D√©finition des Fonctions (identiques √† votre script) ---

def get_wordnet_pos(treebank_tag):
    """Convertit les √©tiquettes POS de Treebank en format WordNet."""
    if treebank_tag.startswith("J"):
        return wordnet.ADJ
    elif treebank_tag.startswith("V"):
        return wordnet.VERB
    elif treebank_tag.startswith("N"):
        return wordnet.NOUN
    elif treebank_tag.startswith("R"):
        return wordnet.ADV
    else:
        # Par d√©faut, consid√©rer comme un nom
        return wordnet.NOUN

def preprocess_text(text, english_words=english_words_set, min_len_word=3, rejoin=False):
    """Pr√©-traite un texte (minuscules, regex, stopwords, mots anglais, lemmatisation)."""
    # S'assurer que le texte est une cha√Æne
    if not isinstance(text, str):
        text = str(text) # Tenter une conversion

    text = text.lower()
    # Garder les espaces pour le split, supprimer les autres caract√®res non-alphab√©tiques
    text = re.sub("[^a-z\s]", "", text)
    # Utiliser la variable globale initialis√©e
    words_list = [word for word in text.split() if word not in stop_words_set]


    # Filtrer par mots anglais seulement si english_words_set est disponible et non vide
    if english_words: # V√©rifie si l'ensemble n'est pas vide
        filtered_corpus = [word for word in words_list if word in english_words]
    else:
        st.warning("Le filtrage par mots anglais est d√©sactiv√© (dictionnaire non charg√©).")
        filtered_corpus = words_list # Ne pas filtrer si le dictionnaire n'est pas l√†

    # V√©rifier si NLTK est charg√© avant de tenter pos_tag et lemmatize
    if not NLTK_DATA_LOADED:
         st.warning("Lemmatisation d√©sactiv√©e (ressources NLTK non charg√©es).")
         final_tokens = [w for w in filtered_corpus if len(w) >= min_len_word]
         if rejoin:
             return " ".join(final_tokens)
         return final_tokens

    try:
        lemmatizer = WordNetLemmatizer()
        tagged_words = pos_tag(filtered_corpus)
        words_list = [
            lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged_words
        ]

        final_tokens = [w for w in words_list if len(w) >= min_len_word]
    except LookupError as e:
        # G√©rer le cas o√π une ressource sp√©cifique manque malgr√© le check initial
        st.error(f"Erreur NLTK pendant le preprocessing (POS tag/lemmatisation): {e}")
        # Retourner les mots filtr√©s avant lemmatisation comme fallback
        words_list = filtered_corpus
    except Exception as e:
        st.error(f"Erreur inattendue pendant le preprocessing: {e}")
        words_list = filtered_corpus # Fallback

    if rejoin:
        return " ".join(final_tokens)

    return final_tokens

# --- Classes Transformer (identiques) ---

class TextPreprocessor(BaseEstimator, TransformerMixin):
    """Transformer Scikit-learn pour le pr√©traitement de texte."""
    def __init__(self, english_words, min_len_word=3, rejoin=True):
        # Utiliser les ensembles globaux charg√©s au d√©but
        self.english_words = english_words_set
        self.min_len_word = min_len_word
        self.rejoin = rejoin

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        # G√©rer les √©ventuelles valeurs non-string dans X
        return [
            preprocess_text(str(doc), self.english_words, self.min_len_word, self.rejoin)
            for doc in X
        ]


class TopicModeler(BaseEstimator, TransformerMixin):
    """Transformer Scikit-learn pour la mod√©lisation de sujets LDA."""
    def __init__(self, num_topics=4, max_features=1000, random_state=0):
        self.num_topics = num_topics
        self.max_features = max_features
        self.random_state = random_state
        self.lda_model = None
        self.count_vectorizer = None
        self.topic_vectors = None
        self.dtm = None # Document-Term Matrix

    def fit(self, X, y=None):
        """Entra√Æne le CountVectorizer et le mod√®le LDA."""
        st.info(f"Entra√Ænement du mod√®le LDA avec {self.num_topics} topics...")
        self.count_vectorizer = CountVectorizer(
            max_df=0.95, min_df=2, # Augmenter min_df peut aider √† la stabilit√©
            max_features=self.max_features,
            stop_words="english" # Redondant si preprocess_text est utilis√© avant, mais sans danger
        )
        try:
            # X est suppos√© √™tre une liste de textes pr√©-trait√©s (strings)
            self.dtm = self.count_vectorizer.fit_transform(X)

            if self.dtm.shape[0] == 0 or self.dtm.shape[1] == 0:
                 st.warning("La matrice DTM est vide apr√®s vectorisation. LDA ne peut pas √™tre entra√Æn√©.")
                 self.lda_model = None
                 return self # Retourner self m√™me en cas d'√©chec partiel

            # V√©rifier si le nombre de features est suffisant pour le nombre de topics
            if self.dtm.shape[1] < self.num_topics:
                 st.warning(f"Nombre de features ({self.dtm.shape[1]}) insuffisant pour {self.num_topics} topics. R√©duction du nombre de topics √† {self.dtm.shape[1]}.")
                 actual_num_topics = self.dtm.shape[1]
            else:
                 actual_num_topics = self.num_topics

            if actual_num_topics < 1:
                 st.error("Aucune feature trouv√©e apr√®s vectorisation. LDA ne peut pas √™tre entra√Æn√©.")
                 self.lda_model = None
                 return self

            self.lda_model = LatentDirichletAllocation(
                n_components=actual_num_topics, # Utiliser le nombre ajust√©
                learning_method="online",
                random_state=self.random_state,
                max_iter=10, # Peut n√©cessiter ajustement
                n_jobs=-1 # Utiliser tous les CPU disponibles
            )
            self.lda_model.fit(self.dtm)
            # Calculer pour les donn√©es d'entra√Ænement pour usage ult√©rieur (ex: t-SNE)
            self.topic_vectors = self.lda_model.transform(self.dtm)
            st.success("Entra√Ænement LDA termin√©.")

        except ValueError as ve:
             st.error(f"Erreur de valeur pendant l'entra√Ænement LDA (peut-√™tre apr√®s preprocessing ?) : {ve}")
             st.warning("V√©rifiez si le preprocessing ne vide pas tous les documents.")
             self.lda_model = None # Assurer que le mod√®le est None
        except Exception as e:
            st.error(f"Erreur inattendue pendant l'entra√Ænement LDA : {e}")
            self.lda_model = None
        return self

    def transform(self, X):
        """Transforme de nouveaux textes en distributions de topics."""
        if self.lda_model is None or self.count_vectorizer is None:
            st.error("Le mod√®le LDA ou le Vectorizer n'a pas √©t√© entra√Æn√© correctement.")
            # Retourner une forme compatible mais vide ou avec des z√©ros
            # Le nombre de topics pourrait √™tre celui demand√© initialement ou ajust√©
            num_output_topics = self.num_topics if self.lda_model is None else self.lda_model.n_components
            return np.zeros((len(X), num_output_topics))

        try:
            # X est suppos√© √™tre une liste de textes pr√©-trait√©s (strings)
            dtm_new = self.count_vectorizer.transform(X)
            topic_vectors_new = self.lda_model.transform(dtm_new)
            return topic_vectors_new
        except Exception as e:
            st.error(f"Erreur pendant la transformation LDA : {e}")
            return np.zeros((len(X), self.lda_model.n_components))

# --- Fonctions de Chargement et Traitement (Mise en cache) ---

@st.cache_data # Cache les donn√©es retourn√©es
def load_data(filepath="negative_reviews5000.csv"):
    """Charge les donn√©es depuis un fichier CSV."""
    try:
        # V√©rifier si le fichier existe avant de lire
        if not os.path.exists(filepath):
             st.error(f"Fichier non trouv√© : {filepath}. Assurez-vous qu'il est dans le bon r√©pertoire.")
             return None
        df = pd.read_csv(filepath)
        # S'assurer que la colonne 'text' existe et ne contient pas que des NaN
        if 'text' not in df.columns:
            st.error(f"La colonne 'text' est manquante dans {filepath}")
            return None
        df = df.dropna(subset=['text'])
        if df.empty:
            st.error(f"Aucune donn√©e valide trouv√©e dans la colonne 'text' de {filepath} apr√®s suppression des NaN.")
            return None
        return df
    except pd.errors.EmptyDataError:
         st.error(f"Le fichier CSV '{filepath}' est vide.")
         return None
    except Exception as e:
        st.error(f"Erreur lors du chargement du fichier CSV '{filepath}' : {e}")
        return None

@st.cache_resource # Cache l'objet pipeline entra√Æn√©
def get_trained_pipeline(data, num_topics=4):
    """Cr√©e et entra√Æne le pipeline de traitement de texte."""
    if data is None or 'text' not in data.columns or data.empty:
         st.error("Donn√©es invalides ou vides fournies pour l'entra√Ænement du pipeline.")
         return None

    # Utiliser TextPreprocessor pour le nettoyage AVANT TopicModeler
    text_pipeline = Pipeline(
        [
            (
                "preprocessing",
                # Passer l'ensemble de mots anglais charg√© globalement
                TextPreprocessor(english_words=english_words_set, min_len_word=3, rejoin=True),
            ),
            ("topic_modeling", TopicModeler(num_topics=num_topics, random_state=42)) # Fixer random_state pour reproductibilit√©
        ]
    )

    try:
        # Entra√Æner sur la colonne 'text' (TextPreprocessor g√®re la conversion en str)
        st.write("D√©but de l'entra√Ænement du pipeline (cela peut prendre un moment)...")
        # L'√©tape 'preprocessing' retourne une liste de strings trait√©s
        # L'√©tape 'topic_modeling' prend cette liste en entr√©e pour fit
        text_pipeline.fit(data["text"])
        st.write("Pipeline entra√Æn√©.")

        # V√©rifier si l'entra√Ænement LDA a r√©ussi dans TopicModeler
        topic_modeler_step = text_pipeline.named_steps.get('topic_modeling')
        if topic_modeler_step is None or topic_modeler_step.lda_model is None:
             st.error("L'√©tape de mod√©lisation de sujet (LDA) dans le pipeline a √©chou√© ou n'a pas produit de mod√®le.")
             return None # Retourner None si l'√©tape cruciale a √©chou√©

    except Exception as e:
        st.error(f"Erreur lors de l'entra√Ænement du pipeline : {e}")
        # Afficher plus de d√©tails si possible, ex: traceback
        # import traceback
        # st.error(traceback.format_exc())
        return None

    return text_pipeline

@st.cache_data # Cache les r√©sultats du t-SNE
def get_tsne_results(_pipeline, _data):
    """Calcule les vecteurs de sujet pour les donn√©es originales et applique t-SNE."""
    if _pipeline is None or _data is None or _data.empty:
        st.warning("Pipeline ou donn√©es manquantes/vides pour t-SNE.")
        return None, None

    topic_modeler = _pipeline.named_steps.get('topic_modeling')
    if topic_modeler is None or topic_modeler.lda_model is None:
        st.warning("Mod√®le LDA non trouv√© dans le pipeline pour t-SNE.")
        return None, None

    # R√©cup√©rer les vecteurs de sujet d√©j√† calcul√©s lors du fit du pipeline
    # Ces vecteurs correspondent aux donn√©es originales (_data['text']) apr√®s preprocessing et LDA
    if hasattr(topic_modeler, 'topic_vectors') and topic_modeler.topic_vectors is not None and len(topic_modeler.topic_vectors) == len(_data):
         original_topic_vectors = topic_modeler.topic_vectors
         st.info("Utilisation des topic vectors pr√©-calcul√©s lors de l'entra√Ænement pour t-SNE.")
    else:
         # Si les vecteurs ne sont pas disponibles (ce qui ne devrait pas arriver si fit a r√©ussi),
         # on pourrait les recalculer, mais cela indique un probl√®me potentiel.
         st.warning("Recalcul des topic vectors pour t-SNE (inattendu)...")
         # Note: pipeline.transform applique toutes les √©tapes, y compris preprocessing
         original_topic_vectors = _pipeline.transform(_data['text'])
         st.info("Recalcul termin√©.")


    if original_topic_vectors is None or original_topic_vectors.shape[0] == 0:
        st.warning("Les vecteurs de sujet pour les donn√©es originales sont vides ou non calcul√©s.")
        return None, None

    st.info("Calcul de la r√©duction t-SNE (cela peut prendre du temps)...")
    # Ajuster perplexity en fonction de la taille des donn√©es si n√©cessaire (doit √™tre < n_samples)
    n_samples = original_topic_vectors.shape[0]
    perplexity_value = min(30, n_samples - 1) # Valeur s√ªre
    if perplexity_value < 5:
         st.warning(f"Tr√®s peu d'√©chantillons ({n_samples}), t-SNE pourrait √™tre moins significatif.")
         perplexity_value = max(1, n_samples - 1) # Minimum 1 si n_samples > 1

    if n_samples <= 1:
         st.warning("t-SNE n√©cessite au moins 2 √©chantillons.")
         return None, None


    tsne_model = TSNE(n_components=2, random_state=42, perplexity=perplexity_value, n_iter=300, init='pca', learning_rate='auto')
    try:
        tsne_vectors = tsne_model.fit_transform(original_topic_vectors)
        st.success("Calcul t-SNE termin√©.")
    except Exception as e:
        st.error(f"Erreur lors du calcul t-SNE : {e}")
        return None, None

    dominant_topics = np.argmax(original_topic_vectors, axis=1)

    # Retourner le mod√®le t-SNE n'est pas utile car il n'a pas de m√©thode transform
    # On retourne juste les vecteurs 2D et les topics dominants
    return tsne_vectors, dominant_topics

# --- Fonctions de Visualisation Adapt√©es ---

def plot_tsne_streamlit(tsne_vectors, dominant_topics, num_topics, predicted_topic_highlight=None):
    """Affiche la visualisation t-SNE avec Plotly. Peut mettre en √©vidence les points d'un topic sp√©cifique."""
    if tsne_vectors is None or dominant_topics is None:
        st.warning("Donn√©es t-SNE manquantes pour la visualisation.")
        return
    if len(tsne_vectors) == 0:
         st.warning("Aucun point √† afficher dans le graphique t-SNE.")
         return

    st.write(f"Visualisation t-SNE des {len(tsne_vectors)} avis originaux.")

    df = pd.DataFrame(tsne_vectors, columns=['x', 'y'])
    # Assurer que dominant_topics a la m√™me longueur que tsne_vectors
    if len(dominant_topics) != len(tsne_vectors):
         st.error("Incoh√©rence entre le nombre de points t-SNE et les topics dominants.")
         return
    df['dominant_topic'] = dominant_topics.astype(str)

    # D√©finir une palette de couleurs
    # Utiliser une palette Plotly standard qui g√®re plus de couleurs
    colors = px.colors.qualitative.Plotly
    # S'assurer que num_topics correspond au nombre r√©el de topics dans le mod√®le LDA
    actual_num_topics = len(set(dominant_topics))
    color_map = {str(i): colors[i % len(colors)] for i in range(actual_num_topics)}

    fig = px.scatter(df, x='x', y='y', color='dominant_topic',
                     color_discrete_map=color_map,
                     title='Visualisation t-SNE des Sujets LDA (Donn√©es Originales)',
                     labels={'dominant_topic': 'Sujet Dominant'},
                     hover_data={'dominant_topic': True, df.index.name: df.index}, # Afficher topic et index au survol
                     template='plotly_white',
                     opacity=0.7) # Rendre les points originaux l√©g√®rement transparents

    # Optionnel : Mettre en √©vidence les points du topic pr√©dit pour le nouveau texte
    if predicted_topic_highlight is not None:
        highlight_topic_str = str(predicted_topic_highlight)
        # Ou plus simplement, ajouter une annotation
        st.info(f"Le topic pr√©dit pour votre texte est le Topic {predicted_topic_highlight}. Les points correspondants dans le graphique ci-dessus repr√©sentent les avis originaux de ce m√™me topic.")
        # On pourrait aussi changer l'apparence des points de ce topic, mais cela peut complexifier le graphique.
        # Exemple (non test√©, n√©cessite ajustement) :
        # fig.for_each_trace(lambda t: t.update(marker=dict(size=10, symbol='diamond')) if t.name == highlight_topic_str else ())


    # Note: Le nouveau point texte n'est PAS ajout√© au graphique t-SNE lui-m√™me.
    fig.update_layout(legend_title_text='Topics')
    st.plotly_chart(fig, use_container_width=True)


def plot_wordcloud_streamlit(pipeline, num_topics, predicted_topic=None):
    """Affiche les nuages de mots pour chaque sujet, en encadrant le sujet pr√©dit."""
    topic_modeler = pipeline.named_steps.get('topic_modeling')
    if topic_modeler is None or topic_modeler.lda_model is None or topic_modeler.count_vectorizer is None:
        st.warning("Mod√®le LDA ou Vectorizer non trouv√© pour g√©n√©rer les Word Clouds.")
        return

    lda_model = topic_modeler.lda_model
    vectorizer = topic_modeler.count_vectorizer
    # V√©rifier si le vectorizer a √©t√© entra√Æn√©
    if not hasattr(vectorizer, 'vocabulary_') or not vectorizer.vocabulary_:
         st.warning("Vectorizer non entra√Æn√©, impossible de r√©cup√©rer les noms de features pour les Word Clouds.")
         return
    try:
        feature_names = vectorizer.get_feature_names_out()
    except Exception as e:
        st.error(f"Erreur lors de la r√©cup√©ration des features du vectorizer: {e}")
        return

    # Utiliser le nombre r√©el de composants du mod√®le LDA
    actual_num_topics = lda_model.n_components

    # D√©terminer la disposition des subplots
    ncols = 2
    nrows = int(np.ceil(actual_num_topics / ncols))
    if nrows == 0 or ncols == 0: # Cas o√π actual_num_topics est 0
         st.warning("Aucun topic trouv√© dans le mod√®le LDA.")
         return

    fig, axes = plt.subplots(nrows, ncols, figsize=(12, nrows * 5), squeeze=False)
    axes = axes.flatten()

    for i in range(actual_num_topics):
        topic_words = lda_model.components_[i]
        # Prendre le top 20 mots ou moins si moins de features
        top_word_indices = topic_words.argsort()[:-min(21, len(feature_names)):-1]
        wc_dict = {
            feature_names[index]: topic_words[index]
            for index in top_word_indices if index < len(feature_names) # V√©rification suppl√©mentaire
        }

        ax = axes[i] # S√©lectionner l'axe courant

        if not wc_dict: # Si aucun mot n'est trouv√© pour ce topic
             ax.set_title(f"Topic {i} (Vide)")
             ax.text(0.5, 0.5, "Aucun mot significatif trouv√©", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)
             ax.axis("off")
             continue

        wc = WordCloud(background_color="white", width=400, height=200, max_words=50) # Limiter max_words
        try:
            wc.generate_from_frequencies(wc_dict)
            ax.set_title(f"Topic {i}")
            ax.imshow(wc, interpolation="bilinear")
            ax.axis("off")

            # Encadrer le topic pr√©dit
            if predicted_topic is not None and i == predicted_topic:
                # Ajouter une bordure rouge
                rect = plt.Rectangle((0,0), 1, 1, fill=False, edgecolor='red', lw=3, transform=ax.transAxes, clip_on=False)
                ax.add_patch(rect)
                # Mettre le titre en √©vidence
                ax.set_title(f"Topic {i} (Votre Texte)", color='red', fontweight='bold')

        except ValueError as ve:
             st.warning(f"Erreur lors de la g√©n√©ration du WordCloud pour le Topic {i}: {ve}")
             ax.set_title(f"Topic {i} (Erreur WC)")
             ax.axis("off")
        except Exception as e:
             st.error(f"Erreur inattendue WordCloud Topic {i}: {e}")
             ax.set_title(f"Topic {i} (Erreur Inattendue)")
             ax.axis("off")

    # Masquer les axes restants si num_topics n'est pas un multiple de ncols
    for j in range(actual_num_topics, nrows * ncols):
         if j < len(axes):
            axes[j].axis("off")

    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Ajuster pour le suptitle
    plt.suptitle("Word Clouds par Sujet LDA", fontsize=16)
    st.pyplot(fig)
    # Fermer la figure pour lib√©rer la m√©moire
    plt.close(fig)


# --- Application Streamlit ---

#st.set_page_config(layout="wide") # Mettre en page large par d√©faut
st.title("Analyse de Sujets (Topics) LDA pour Avis Clients")

# --- Chargement et Entra√Ænement (mis en cache) ---
data_load_state = st.text("Chargement des donn√©es...")
df_reviews = load_data() # Utilise le chemin par d√©faut "negative_reviews5000.csv"
if df_reviews is not None:
    data_load_state.text(f"Chargement des donn√©es... Termin√©! ({len(df_reviews)} avis charg√©s)")
else:
    data_load_state.error("Chargement des donn√©es √©chou√©.")
    st.stop() # Arr√™ter si les donn√©es ne peuvent pas √™tre charg√©es

st.subheader("Aper√ßu des donn√©es brutes")
st.dataframe(df_reviews.head())

# D√©finir le nombre de topics ici
# Id√©alement, ce nombre serait d√©termin√© par une analyse pr√©alable (ex: score de coh√©rence)
# Pour l'exemple, on fixe √† 4
NUM_TOPICS = 4
st.sidebar.info(f"Nombre de topics configur√© : {NUM_TOPICS}")

# Obtenir le pipeline entra√Æn√© (depuis le cache si possible)
pipeline = get_trained_pipeline(df_reviews, num_topics=NUM_TOPICS)

# V√©rifier si le pipeline a √©t√© entra√Æn√© correctement
if pipeline is None:
    st.error("Le pipeline d'analyse n'a pas pu √™tre entra√Æn√©. V√©rifiez les logs et les donn√©es d'entr√©e.")
    st.stop() # Arr√™ter si le pipeline √©choue

# V√©rifier le nombre r√©el de topics dans le mod√®le entra√Æn√©
topic_modeler_step = pipeline.named_steps.get('topic_modeling')
if topic_modeler_step and topic_modeler_step.lda_model:
     ACTUAL_NUM_TOPICS = topic_modeler_step.lda_model.n_components
     if ACTUAL_NUM_TOPICS != NUM_TOPICS:
          st.sidebar.warning(f"Le nombre de topics a √©t√© ajust√© √† {ACTUAL_NUM_TOPICS} en raison des donn√©es.")
else:
     # Si le mod√®le n'est pas l√†, on ne peut pas continuer
     st.error("Le mod√®le LDA n'a pas √©t√© trouv√© dans le pipeline apr√®s l'entra√Ænement.")
     st.stop()


# Obtenir les r√©sultats t-SNE (depuis le cache si possible)
# Passer le pipeline et les donn√©es originales
tsne_vectors, dominant_topics = get_tsne_results(pipeline, df_reviews)

st.sidebar.success("Mod√®le pr√™t pour la pr√©diction.")

# --- Interface Utilisateur ---
st.divider()
st.header("Tester avec votre propre texte")
user_text = st.text_area("Entrez un avis (en anglais) ici :", height=150, placeholder="Ex: The product was amazing, great quality and fast delivery!")

if st.button("Analyser le Sujet"):
    if user_text and user_text.strip(): # V√©rifier si le texte n'est pas vide ou juste des espaces
        with st.spinner("Analyse en cours..."):
            # 1. Pr√©dire la distribution des topics pour le nouveau texte
            #    Utiliser directement pipeline.transform qui applique toutes les √©tapes
            #    Assurer que l'entr√©e est une liste ou similaire
            topic_distribution_new = pipeline.transform([user_text])

            if topic_distribution_new is not None and topic_distribution_new.size > 0:
                # Trouver l'index du topic le plus probable
                predicted_topic = np.argmax(topic_distribution_new, axis=1)[0]
                probabilities = topic_distribution_new[0]

                st.subheader("R√©sultats de l'Analyse")
                st.write(f"**Sujet (Topic) Pr√©dit :** Topic {predicted_topic}")
                st.write("**Probabilit√©s par Sujet :**")
                # Cr√©er un DataFrame pour un affichage clair des probabilit√©s
                prob_df = pd.DataFrame(
                    probabilities,
                    index=[f"Topic {i}" for i in range(ACTUAL_NUM_TOPICS)], # Utiliser le nombre r√©el de topics
                    columns=["Probabilit√©"]
                )
                st.dataframe(prob_df.style.format("{:.2%}")) # Afficher en pourcentage

                # Afficher les mots cl√©s du topic pr√©dit
                # R√©cup√©rer le mod√®le et le vectorizer depuis le pipeline
                topic_modeler = pipeline.named_steps.get('topic_modeling')
                if topic_modeler and topic_modeler.lda_model and topic_modeler.count_vectorizer:
                    vectorizer = topic_modeler.count_vectorizer
                    lda_model = topic_modeler.lda_model
                    # V√©rifier si le vectorizer a des features
                    if hasattr(vectorizer, 'vocabulary_') and vectorizer.vocabulary_:
                        feature_names = vectorizer.get_feature_names_out()
                        n_top_words = 10 # Nombre de mots cl√©s √† afficher
                        st.write(f"**Mots cl√©s principaux pour le Topic {predicted_topic} :**")
                        try:
                            # S'assurer que predicted_topic est un index valide
                            if 0 <= predicted_topic < lda_model.n_components:
                                topic_loadings = lda_model.components_[predicted_topic]
                                top_words_indices = topic_loadings.argsort()[:-n_top_words - 1:-1]
                                # V√©rifier les indices avant d'acc√©der √† feature_names
                                top_words = [
                                    feature_names[i] for i in top_words_indices if i < len(feature_names)
                                ]
                                st.info(", ".join(top_words))
                            else:
                                st.warning(f"Index de topic pr√©dit ({predicted_topic}) invalide.")
                        except IndexError:
                             st.warning(f"Impossible de r√©cup√©rer les mots cl√©s pour le topic {predicted_topic} (Index hors limites).")
                        except Exception as e:
                             st.warning(f"Erreur lors de la r√©cup√©ration des mots cl√©s : {e}")
                    else:
                         st.warning("Vectorizer non entra√Æn√©, impossible d'afficher les mots cl√©s.")
                else:
                     st.warning("Impossible de r√©cup√©rer le mod√®le LDA ou le Vectorizer depuis le pipeline.")


                st.divider()
                st.subheader("Visualisations")

                # 2. Visualisation t-SNE (sans projeter le nouveau point)
                if tsne_vectors is not None and dominant_topics is not None:
                    # Passer le topic pr√©dit pour mise en √©vidence potentielle
                    try:
                        plot_tsne_streamlit(tsne_vectors, dominant_topics, ACTUAL_NUM_TOPICS, predicted_topic_highlight=predicted_topic)
                    except AttributeError as ae: # Capture l'erreur sp√©cifique si on essaie encore d'utiliser transform (ne devrait plus arriver)
                         st.warning(f"Impossible de transformer le nouveau point pour t-SNE (peut-√™tre dimensions incompatibles): {ve}")
                    except Exception as e:
                         st.error(f"Erreur lors de la projection t-SNE du nouveau point : {e}")

                else:
                    st.warning("Visualisation t-SNE non disponible (calcul √©chou√© ou donn√©es manquantes).")

                # 3. Visualisation Word Cloud en mettant en √©vidence le topic pr√©dit
                plot_wordcloud_streamlit(pipeline, ACTUAL_NUM_TOPICS, predicted_topic=predicted_topic)

            else:
                st.error("La pr√©diction du topic a √©chou√© pour le texte fourni. V√©rifiez le preprocessing ou le mod√®le.")
    else:
        st.warning("Veuillez entrer du texte √† analyser.")


st.sidebar.markdown("---")
st.sidebar.header("Informations")
st.sidebar.markdown("""
Cette application utilise le mod√®le LDA (Latent Dirichlet Allocation) pour identifier
les sujets cach√©s dans un corpus d'avis clients n√©gatifs.

- Entrez un nouvel avis pour voir quel sujet lui est attribu√©.
- La visualisation t-SNE montre la proximit√© des documents originaux en fonction de leurs sujets.
- Les Word Clouds affichent les mots les plus repr√©sentatifs de chaque sujet.
""")

# Optionnel: Ajouter un pied de page
st.markdown("---")
st.caption("Application d√©velopp√©e avec Streamlit, NLTK, Scikit-learn, Plotly, Matplotlib.")


#=======================images=====================

import streamlit as st
import cv2
import numpy as np
from PIL import Image # Pour g√©rer les images t√©l√©charg√©es par Streamlit
import joblib # Pour charger les objets scikit-learn sauvegard√©s
import os

# Imports TensorFlow/Keras
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.applications.efficientnet import preprocess_input

# --- Configuration ---
# Chemins vers les composants du mod√®le sauvegard√©s (doivent correspondre √† ceux de Nmeth.py)
SCALER_PATH = 'scaler.joblib'
PCA_PATH = 'pca.joblib'
MODEL_PATH = 'random_forest_model.joblib'
LABEL_ENCODER_PATH = 'label_encoder.joblib' # Optionnel, seulement si utilis√© lors de l'entra√Ænement

TSNE_PLOT_IMAGE_PATH = 'tsne_visualization.png' # Doit correspondre √† Nmeth.py

EFFICIENTNET_INPUT_SIZE = (224, 224) # Doit correspondre √† la taille utilis√©e lors de l'entra√Ænement

# --- Chargement des Mod√®les et Transformateurs ---

# Extracteur de caract√©ristiques (EfficientNetB0) - charg√© une seule fois
# Pas besoin de sauvegarder/charger sp√©cifiquement si vous utilisez les poids 'imagenet' et non fine-tun√©.
@st.cache_resource # Mise en cache pour am√©liorer les performances apr√®s le premier chargement
def load_feature_extractor():
    model = EfficientNetB0(include_top=False, weights='imagenet', pooling='avg')
    return model

feature_extractor = load_feature_extractor()

# Fonction pour charger les objets scikit-learn
@st.cache_resource
def load_sklearn_object(path):
    if os.path.exists(path):
        try:
            return joblib.load(path)
        except Exception as e:
            st.error(f"Erreur lors du chargement de l'objet depuis {path}: {e}")
            return None
    else:
        # Ne pas afficher d'erreur ici si le fichier est optionnel (comme label_encoder)
        # La logique de pr√©diction g√©rera si les objets essentiels manquent.
        if path != LABEL_ENCODER_PATH: # Avertir seulement pour les fichiers essentiels
             st.warning(f"Avertissement : Fichier non trouv√© √† {path}. La pr√©diction pourrait ne pas fonctionner.")
        return None

scaler = load_sklearn_object(SCALER_PATH)
pca = load_sklearn_object(PCA_PATH)
classifier = load_sklearn_object(MODEL_PATH)
label_encoder = load_sklearn_object(LABEL_ENCODER_PATH) # Sera None si le fichier n'existe pas ou n'a pas √©t√© utilis√©

# --- Fonction de Pr√©diction ---
def predict_image_topic(image_array_bgr, feature_extractor_model, scaler_obj, pca_obj, classifier_model, label_encoder_obj=None):
    """
    Pr√©dit le topic d'une image donn√©e (attend un array OpenCV BGR).
    """
    if image_array_bgr is None:
        return "Erreur: Image non valide."

    try:
        # Pr√©traitement de l'image (identique √† celui de l'entra√Ænement)
        img_rgb = cv2.cvtColor(image_array_bgr, cv2.COLOR_BGR2RGB) # Conversion en RGB pour EfficientNet
        img_resized = cv2.resize(img_rgb, EFFICIENTNET_INPUT_SIZE)
        img_expanded = np.expand_dims(img_resized, axis=0) # Ajout de la dimension du batch
        img_preprocessed = preprocess_input(img_expanded) # Pr√©traitement sp√©cifique √† EfficientNet

        # 1. Extraction des caract√©ristiques
        features_new_image = feature_extractor_model.predict(img_preprocessed, verbose=0)[0]
        features_new_image_reshaped = features_new_image.reshape(1, -1) # Pour scaler et pca

        # V√©rification que les composants essentiels sont charg√©s
        if scaler_obj is None: return "Erreur: Scaler non charg√©. Impossible de pr√©dire."
        if pca_obj is None: return "Erreur: PCA non charg√©. Impossible de pr√©dire."
        if classifier_model is None: return "Erreur: Mod√®le de classification non charg√©. Impossible de pr√©dire."

        # 2. Standardisation
        features_new_image_std = scaler_obj.transform(features_new_image_reshaped)

        # 3. Transformation PCA
        features_new_image_pca = pca_obj.transform(features_new_image_std)

        # 4. Pr√©diction
        prediction_numeric = classifier_model.predict(features_new_image_pca)[0]

        # 5. D√©codage de l'√©tiquette (si un encodeur a √©t√© utilis√© et est ajust√©)
        if label_encoder_obj and hasattr(label_encoder_obj, 'classes_'):
            return label_encoder_obj.inverse_transform([prediction_numeric])[0]
        else:
            return str(prediction_numeric) # Retourne la pr√©diction num√©rique comme cha√Æne

    except Exception as e:
        return f"Erreur lors de la pr√©diction : {e}"

# --- Interface Utilisateur Streamlit ---
#st.set_page_config(page_title="Pr√©diction de Topic d'Image", layout="centered")
st.title("üñºÔ∏è Pr√©diction de Topic d'Image")

st.write("""
    T√©l√©chargez une image (JPG, JPEG, PNG) et l'application pr√©dira son topic
    en utilisant un pipeline de Machine Learning pr√©-entra√Æn√©.
""")

uploaded_file = st.file_uploader("Choisissez une image...", type=["jpg", "jpeg", "png"])

if uploaded_file is not None:
    pil_image = Image.open(uploaded_file)
    st.image(pil_image, caption="Image t√©l√©charg√©e", use_column_width=True)
    
    # Conversion de l'image PIL (RGB) en format OpenCV (BGR) pour la fonction de pr√©diction
    opencv_image_rgb = np.array(pil_image)
    opencv_image_bgr = cv2.cvtColor(opencv_image_rgb, cv2.COLOR_RGB2BGR)

    with st.spinner("Pr√©diction en cours..."):
        prediction = predict_image_topic(
            opencv_image_bgr,
            feature_extractor,
            scaler,
            pca,
            classifier,
            label_encoder_obj=label_encoder
        )
        st.subheader(f"üîç Topic Pr√©dit : **{prediction}**")

st.sidebar.header("‚ÑπÔ∏è √Ä propos")
st.sidebar.info(f"""
    Cette application utilise EfficientNetB0 pour l'extraction de caract√©ristiques, suivi d'un StandardScaler, d'une PCA, et d'un classifieur RandomForest.
    Les mod√®les et transformateurs scikit-learn sont charg√©s depuis :
    - `{SCALER_PATH}`
    - `{PCA_PATH}`
    - `{MODEL_PATH}`
    - `{LABEL_ENCODER_PATH}` (si utilis√©)
""")
st.sidebar.header("üìä Visualisation t-SNE")
if os.path.exists(TSNE_PLOT_IMAGE_PATH):
    try:
        st.sidebar.image(TSNE_PLOT_IMAGE_PATH, caption="Visualisation t-SNE des caract√©ristiques (entra√Ænement)")
    except Exception as e:
        st.sidebar.error(f"Erreur lors du chargement de l'image t-SNE: {e}")
else:
    st.sidebar.warning(f"L'image de la visualisation t-SNE ({TSNE_PLOT_IMAGE_PATH}) n'a pas √©t√© trouv√©e. Veuillez ex√©cuter le script d'entra√Ænement pour la g√©n√©rer.")
